"""
Comprehensive Deployment Validation for AI Trading System
Tests all components before production deployment
"""

import asyncio
import sys
import traceback
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
import json
import logging
from pathlib import Path
import numpy as np
import pandas as pd
from dataclasses import dataclass
import time
import warnings
warnings.filterwarnings('ignore')

# Import all AI components
from ai_integration_framework import AIIntegrationFramework
from advanced_ml_models import AdvancedMLModels
from sentiment_analysis_engine import SentimentAnalysisEngine
from backtesting_framework import BacktestEngine, BacktestConfig
from model_validation_testing import ModelValidationFramework
from cybersecurity_framework import SecureTradingFramework
from secure_autonomous_trader import SecureAgenticElizaOS
from portfolio_visualizer import PortfolioVisualizer
from secure_config_manager import SecureConfigManager
from advanced_analytics_engine import AdvancedAnalyticsEngine

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('deployment_validation.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class ValidationTest:
    """Individual validation test"""
    name: str
    description: str
    category: str
    critical: bool = False
    passed: bool = False
    error: Optional[str] = None
    duration: float = 0.0
    metadata: Dict[str, Any] = None

@dataclass
class DeploymentValidationReport:
    """Comprehensive deployment validation report"""
    timestamp: datetime
    overall_status: str  # 'PASS', 'FAIL', 'WARNING'
    total_tests: int
    passed_tests: int
    failed_tests: int
    critical_failures: int
    
    tests_by_category: Dict[str, List[ValidationTest]]
    
    performance_metrics: Dict[str, float]
    system_requirements: Dict[str, bool]
    security_assessment: Dict[str, Any]
    
    recommendations: List[str]
    deployment_ready: bool

class DeploymentValidator:
    """Comprehensive deployment validation system"""
    
    def __init__(self):
        self.test_results = []
        self.start_time = None
        self.validation_config = self._load_validation_config()
        
        # Test categories
        self.test_categories = [
            'system_requirements',
            'component_initialization',
            'data_connectivity',
            'model_validation',
            'security_tests',
            'performance_tests',
            'integration_tests',
            'end_to_end_tests'
        ]
        
        logger.info("üîç Deployment Validator initialized")
        logger.info(f"   Test categories: {len(self.test_categories)}")
    
    def _load_validation_config(self) -> Dict[str, Any]:
        """Load validation configuration"""
        return {
            'test_timeout': 300,  # 5 minutes per test
            'performance_thresholds': {
                'ml_training_time': 120,  # 2 minutes max
                'signal_generation_time': 5,  # 5 seconds max
                'validation_score_min': 0.7,
                'memory_usage_max': 2048,  # 2GB max
                'cpu_usage_max': 80  # 80% max
            },
            'required_components': [
                'ml_models',
                'sentiment_analysis',
                'backtesting',
                'model_validation',
                'security_framework',
                'portfolio_visualizer',
                'config_manager',
                'analytics_engine'
            ],
            'test_assets': ['BTC-USD', 'ETH-USD', 'AAPL'],
            'min_data_points': 100
        }
    
    async def run_comprehensive_validation(self) -> DeploymentValidationReport:
        """Run comprehensive deployment validation"""
        logger.info("üöÄ Starting comprehensive deployment validation...")
        self.start_time = time.time()
        
        try:
            # Run all test categories
            for category in self.test_categories:
                logger.info(f"üî¨ Running {category} tests...")
                await self._run_category_tests(category)
            
            # Generate final report
            report = self._generate_deployment_report()
            
            # Save report
            self._save_validation_report(report)
            
            return report
            
        except Exception as e:
            logger.error(f"‚ùå Validation failed: {e}")
            return self._create_failed_report(str(e))
    
    async def _run_category_tests(self, category: str):
        """Run tests for a specific category"""
        if category == 'system_requirements':
            await self._test_system_requirements()
        elif category == 'component_initialization':
            await self._test_component_initialization()
        elif category == 'data_connectivity':
            await self._test_data_connectivity()
        elif category == 'model_validation':
            await self._test_model_validation()
        elif category == 'security_tests':
            await self._test_security()
        elif category == 'performance_tests':
            await self._test_performance()
        elif category == 'integration_tests':
            await self._test_integration()
        elif category == 'end_to_end_tests':
            await self._test_end_to_end()
    
    async def _test_system_requirements(self):
        """Test system requirements"""
        # Python version test
        await self._run_test(
            name="python_version",
            description="Check Python version compatibility",
            category="system_requirements",
            critical=True,
            test_func=self._check_python_version
        )
        
        # Required packages test
        await self._run_test(
            name="package_dependencies",
            description="Check required package dependencies",
            category="system_requirements",
            critical=True,
            test_func=self._check_package_dependencies
        )
        
        # System resources test
        await self._run_test(
            name="system_resources",
            description="Check system memory and CPU",
            category="system_requirements",
            critical=False,
            test_func=self._check_system_resources
        )
        
        # File system permissions test
        await self._run_test(
            name="file_permissions",
            description="Check file system permissions",
            category="system_requirements",
            critical=True,
            test_func=self._check_file_permissions
        )
    
    async def _test_component_initialization(self):
        """Test component initialization"""
        components = [
            ("ml_models", AdvancedMLModels, "ML Models component"),
            ("sentiment_engine", SentimentAnalysisEngine, "Sentiment Analysis component"),
            ("backtesting", BacktestEngine, "Backtesting component"),
            ("validator", ModelValidationFramework, "Model Validation component"),
            ("security", SecureTradingFramework, "Security Framework component"),
            ("visualizer", PortfolioVisualizer, "Portfolio Visualizer component"),
            ("config_manager", SecureConfigManager, "Config Manager component"),
            ("analytics", AdvancedAnalyticsEngine, "Analytics Engine component")
        ]
        
        for comp_name, comp_class, description in components:
            await self._run_test(
                name=f"init_{comp_name}",
                description=f"Initialize {description}",
                category="component_initialization",
                critical=True,
                test_func=lambda cls=comp_class: self._test_component_init(cls)
            )
    
    async def _test_data_connectivity(self):
        """Test data connectivity"""
        # Market data connectivity
        await self._run_test(
            name="market_data_connectivity",
            description="Test market data fetching",
            category="data_connectivity",
            critical=True,
            test_func=self._test_market_data_connectivity
        )
        
        # News data connectivity
        await self._run_test(
            name="news_data_connectivity",
            description="Test news data fetching",
            category="data_connectivity",
            critical=False,
            test_func=self._test_news_data_connectivity
        )
        
        # API rate limits test
        await self._run_test(
            name="api_rate_limits",
            description="Test API rate limit handling",
            category="data_connectivity",
            critical=False,
            test_func=self._test_api_rate_limits
        )
    
    async def _test_model_validation(self):
        """Test model validation"""
        # Model training test
        await self._run_test(
            name="model_training",
            description="Test ML model training",
            category="model_validation",
            critical=True,
            test_func=self._test_model_training
        )
        
        # Model validation test
        await self._run_test(
            name="model_validation",
            description="Test model validation framework",
            category="model_validation",
            critical=True,
            test_func=self._test_model_validation_framework
        )
        
        # Model prediction test
        await self._run_test(
            name="model_prediction",
            description="Test model prediction generation",
            category="model_validation",
            critical=True,
            test_func=self._test_model_prediction
        )
    
    async def _test_security(self):
        """Test security framework"""
        # Security initialization
        await self._run_test(
            name="security_initialization",
            description="Test security framework initialization",
            category="security_tests",
            critical=True,
            test_func=self._test_security_initialization
        )
        
        # Encryption test
        await self._run_test(
            name="encryption_test",
            description="Test data encryption/decryption",
            category="security_tests",
            critical=True,
            test_func=self._test_encryption
        )
        
        # Authentication test
        await self._run_test(
            name="authentication_test",
            description="Test authentication system",
            category="security_tests",
            critical=True,
            test_func=self._test_authentication
        )
    
    async def _test_performance(self):
        """Test system performance"""
        # Memory usage test
        await self._run_test(
            name="memory_usage",
            description="Test system memory usage",
            category="performance_tests",
            critical=False,
            test_func=self._test_memory_usage
        )
        
        # Processing speed test
        await self._run_test(
            name="processing_speed",
            description="Test signal processing speed",
            category="performance_tests",
            critical=False,
            test_func=self._test_processing_speed
        )
        
        # Concurrent processing test
        await self._run_test(
            name="concurrent_processing",
            description="Test concurrent processing capability",
            category="performance_tests",
            critical=False,
            test_func=self._test_concurrent_processing
        )
    
    async def _test_integration(self):
        """Test component integration"""
        # AI framework integration
        await self._run_test(
            name="ai_framework_integration",
            description="Test AI integration framework",
            category="integration_tests",
            critical=True,
            test_func=self._test_ai_framework_integration
        )
        
        # Signal generation integration
        await self._run_test(
            name="signal_generation_integration",
            description="Test integrated signal generation",
            category="integration_tests",
            critical=True,
            test_func=self._test_signal_generation_integration
        )
        
        # Backtesting integration
        await self._run_test(
            name="backtesting_integration",
            description="Test backtesting integration",
            category="integration_tests",
            critical=True,
            test_func=self._test_backtesting_integration
        )
    
    async def _test_end_to_end(self):
        """Test end-to-end functionality"""
        # Full trading cycle test
        await self._run_test(
            name="full_trading_cycle",
            description="Test complete trading cycle",
            category="end_to_end_tests",
            critical=True,
            test_func=self._test_full_trading_cycle
        )
        
        # Autonomous system test
        await self._run_test(
            name="autonomous_system",
            description="Test autonomous trading system",
            category="end_to_end_tests",
            critical=True,
            test_func=self._test_autonomous_system
        )
    
    async def _run_test(self, name: str, description: str, category: str, 
                       critical: bool, test_func: callable):
        """Run individual test"""
        logger.info(f"   üî¨ Running {name}...")
        
        test_start = time.time()
        test = ValidationTest(
            name=name,
            description=description,
            category=category,
            critical=critical,
            metadata={}
        )
        
        try:
            # Run test with timeout
            result = await asyncio.wait_for(
                test_func(),
                timeout=self.validation_config['test_timeout']
            )
            
            test.passed = True
            test.metadata = result if isinstance(result, dict) else {}
            
            logger.info(f"      ‚úÖ {name} passed")
            
        except asyncio.TimeoutError:
            test.passed = False
            test.error = "Test timed out"
            logger.error(f"      ‚ùå {name} timed out")
            
        except Exception as e:
            test.passed = False
            test.error = str(e)
            logger.error(f"      ‚ùå {name} failed: {e}")
        
        test.duration = time.time() - test_start
        self.test_results.append(test)
    
    # Test implementation methods
    async def _check_python_version(self) -> Dict[str, Any]:
        """Check Python version"""
        if sys.version_info < (3, 8):
            raise Exception(f"Python 3.8+ required, got {sys.version_info}")
        return {"version": sys.version, "compatible": True}
    
    async def _check_package_dependencies(self) -> Dict[str, Any]:
        """Check required packages"""
        required_packages = [
            'numpy', 'pandas', 'scikit-learn', 'matplotlib', 'seaborn',
            'yfinance', 'requests', 'openai', 'torch', 'plotly', 'dash'
        ]
        
        missing_packages = []
        for package in required_packages:
            try:
                __import__(package)
            except ImportError:
                missing_packages.append(package)
        
        if missing_packages:
            raise Exception(f"Missing packages: {missing_packages}")
        
        return {"required_packages": required_packages, "all_available": True}
    
    async def _check_system_resources(self) -> Dict[str, Any]:
        """Check system resources"""
        try:
            import psutil
            
            # Memory check
            memory = psutil.virtual_memory()
            memory_gb = memory.total / (1024**3)
            
            # CPU check
            cpu_count = psutil.cpu_count()
            cpu_percent = psutil.cpu_percent(interval=1)
            
            if memory_gb < 4:
                raise Exception(f"Insufficient memory: {memory_gb:.1f}GB (4GB+ recommended)")
            
            return {
                "memory_gb": memory_gb,
                "cpu_count": cpu_count,
                "cpu_percent": cpu_percent,
                "adequate": True
            }
            
        except ImportError:
            return {"psutil_available": False, "adequate": True}
    
    async def _check_file_permissions(self) -> Dict[str, Any]:
        """Check file system permissions"""
        test_file = Path("test_permissions.tmp")
        
        try:
            # Test write
            test_file.write_text("test")
            
            # Test read
            content = test_file.read_text()
            
            # Test delete
            test_file.unlink()
            
            return {"write": True, "read": True, "delete": True}
            
        except Exception as e:
            if test_file.exists():
                test_file.unlink()
            raise Exception(f"File permission error: {e}")
    
    async def _test_component_init(self, component_class) -> Dict[str, Any]:
        """Test component initialization"""
        try:
            if component_class == BacktestEngine:
                # BacktestEngine needs config
                config = BacktestConfig(
                    start_date='2023-01-01',
                    end_date='2023-12-31'
                )
                component = component_class(config)
            else:
                component = component_class()
            
            return {"initialized": True, "type": str(type(component))}
            
        except Exception as e:
            raise Exception(f"Component initialization failed: {e}")
    
    async def _test_market_data_connectivity(self) -> Dict[str, Any]:
        """Test market data connectivity"""
        try:
            import yfinance as yf
            
            # Test data fetching
            ticker = yf.Ticker('AAPL')
            data = ticker.history(period='5d')
            
            if data.empty:
                raise Exception("No market data received")
            
            return {
                "data_points": len(data),
                "columns": list(data.columns),
                "date_range": f"{data.index[0]} to {data.index[-1]}"
            }
            
        except Exception as e:
            raise Exception(f"Market data connectivity failed: {e}")
    
    async def _test_news_data_connectivity(self) -> Dict[str, Any]:
        """Test news data connectivity"""
        try:
            import feedparser
            
            # Test RSS feed
            feed = feedparser.parse('https://feeds.bloomberg.com/markets/news.rss')
            
            if not feed.entries:
                raise Exception("No news data received")
            
            return {
                "articles_count": len(feed.entries),
                "feed_title": feed.feed.get('title', 'Unknown'),
                "latest_article": feed.entries[0].get('title', 'Unknown')
            }
            
        except Exception as e:
            # News connectivity is not critical
            return {"available": False, "error": str(e)}
    
    async def _test_api_rate_limits(self) -> Dict[str, Any]:
        """Test API rate limit handling"""
        # Placeholder - would test actual API rate limits
        return {"rate_limits_handled": True}
    
    async def _test_model_training(self) -> Dict[str, Any]:
        """Test ML model training"""
        try:
            ml_models = AdvancedMLModels()
            
            # Use a simple test case
            models = ml_models.train_all_models('AAPL')
            
            if not models:
                raise Exception("No models trained")
            
            return {
                "models_trained": len(models),
                "model_types": list(models.keys())
            }
            
        except Exception as e:
            raise Exception(f"Model training failed: {e}")
    
    async def _test_model_validation_framework(self) -> Dict[str, Any]:
        """Test model validation framework"""
        try:
            from sklearn.linear_model import LinearRegression
            
            # Create test data
            X = np.random.randn(100, 5)
            y = np.random.randn(100)
            
            # Create and train model
            model = LinearRegression()
            model.fit(X, y)
            
            # Validate model
            validator = ModelValidationFramework()
            report = validator.validate_model(model, X, y, 'test_model')
            
            return {
                "validation_completed": True,
                "overall_score": report.overall_score,
                "tests_passed": report.passed_tests,
                "total_tests": report.total_tests
            }
            
        except Exception as e:
            raise Exception(f"Model validation failed: {e}")
    
    async def _test_model_prediction(self) -> Dict[str, Any]:
        """Test model prediction generation"""
        try:
            ml_models = AdvancedMLModels()
            
            # Get test data
            data = ml_models.fetch_market_data('AAPL', period='1mo')
            
            if data.empty:
                raise Exception("No data for prediction test")
            
            # Train models first
            models = ml_models.train_all_models('AAPL')
            
            # Generate predictions
            predictions = ml_models.predict('AAPL', data)
            
            return {
                "predictions_generated": len(predictions),
                "prediction_types": [p.model_type for p in predictions]
            }
            
        except Exception as e:
            raise Exception(f"Model prediction failed: {e}")
    
    async def _test_security_initialization(self) -> Dict[str, Any]:
        """Test security framework initialization"""
        try:
            security = SecureTradingFramework()
            
            return {"security_initialized": True}
            
        except Exception as e:
            raise Exception(f"Security initialization failed: {e}")
    
    async def _test_encryption(self) -> Dict[str, Any]:
        """Test encryption/decryption"""
        try:
            from cryptography.fernet import Fernet
            
            # Generate key
            key = Fernet.generate_key()
            f = Fernet(key)
            
            # Test encryption/decryption
            test_data = "test_encryption_data"
            encrypted = f.encrypt(test_data.encode())
            decrypted = f.decrypt(encrypted).decode()
            
            if decrypted != test_data:
                raise Exception("Encryption/decryption failed")
            
            return {"encryption_working": True}
            
        except Exception as e:
            raise Exception(f"Encryption test failed: {e}")
    
    async def _test_authentication(self) -> Dict[str, Any]:
        """Test authentication system"""
        # Placeholder for authentication testing
        return {"authentication_working": True}
    
    async def _test_memory_usage(self) -> Dict[str, Any]:
        """Test memory usage"""
        try:
            import psutil
            
            process = psutil.Process()
            memory_info = process.memory_info()
            memory_mb = memory_info.rss / (1024 * 1024)
            
            threshold = self.validation_config['performance_thresholds']['memory_usage_max']
            
            if memory_mb > threshold:
                raise Exception(f"Memory usage too high: {memory_mb:.1f}MB (max: {threshold}MB)")
            
            return {"memory_usage_mb": memory_mb, "within_limits": True}
            
        except ImportError:
            return {"psutil_available": False, "within_limits": True}
    
    async def _test_processing_speed(self) -> Dict[str, Any]:
        """Test processing speed"""
        try:
            # Time a simple ML prediction
            start_time = time.time()
            
            ml_models = AdvancedMLModels()
            data = ml_models.fetch_market_data('AAPL', period='1mo')
            
            if not data.empty:
                models = ml_models.train_all_models('AAPL')
                predictions = ml_models.predict('AAPL', data)
            
            processing_time = time.time() - start_time
            
            threshold = self.validation_config['performance_thresholds']['signal_generation_time']
            
            if processing_time > threshold:
                raise Exception(f"Processing too slow: {processing_time:.1f}s (max: {threshold}s)")
            
            return {"processing_time_s": processing_time, "within_limits": True}
            
        except Exception as e:
            raise Exception(f"Processing speed test failed: {e}")
    
    async def _test_concurrent_processing(self) -> Dict[str, Any]:
        """Test concurrent processing"""
        try:
            # Test concurrent signal generation
            from concurrent.futures import ThreadPoolExecutor
            
            def generate_dummy_signal(asset):
                time.sleep(0.1)  # Simulate processing
                return f"signal_{asset}"
            
            assets = ['AAPL', 'GOOGL', 'MSFT']
            
            start_time = time.time()
            with ThreadPoolExecutor(max_workers=3) as executor:
                futures = [executor.submit(generate_dummy_signal, asset) for asset in assets]
                results = [future.result() for future in futures]
            
            processing_time = time.time() - start_time
            
            return {
                "concurrent_processing": True,
                "processing_time_s": processing_time,
                "results": results
            }
            
        except Exception as e:
            raise Exception(f"Concurrent processing test failed: {e}")
    
    async def _test_ai_framework_integration(self) -> Dict[str, Any]:
        """Test AI framework integration"""
        try:
            framework = AIIntegrationFramework()
            
            # Test initialization
            success = await framework.initialize_system()
            
            if not success:
                raise Exception("AI framework initialization failed")
            
            return {"framework_initialized": True}
            
        except Exception as e:
            raise Exception(f"AI framework integration failed: {e}")
    
    async def _test_signal_generation_integration(self) -> Dict[str, Any]:
        """Test signal generation integration"""
        try:
            framework = AIIntegrationFramework()
            
            # Initialize system
            await framework.initialize_system()
            
            # Generate test signals
            signals = []
            for asset in ['AAPL', 'GOOGL']:
                signal = await framework.generate_ai_signal(asset)
                signals.append(signal)
            
            return {
                "signals_generated": len(signals),
                "signal_types": [s.combined_action for s in signals]
            }
            
        except Exception as e:
            raise Exception(f"Signal generation integration failed: {e}")
    
    async def _test_backtesting_integration(self) -> Dict[str, Any]:
        """Test backtesting integration"""
        try:
            config = BacktestConfig(
                start_date='2023-01-01',
                end_date='2023-06-30',
                initial_capital=100000
            )
            
            engine = BacktestEngine(config)
            
            # Test with simple strategy
            def simple_strategy(current_date, asset_data, portfolio_value, current_positions, params):
                return []  # No signals for test
            
            results = engine.backtest_strategy(
                strategy_func=simple_strategy,
                assets=['AAPL'],
                strategy_params={}
            )
            
            return {
                "backtesting_completed": True,
                "has_results": len(results) > 0
            }
            
        except Exception as e:
            raise Exception(f"Backtesting integration failed: {e}")
    
    async def _test_full_trading_cycle(self) -> Dict[str, Any]:
        """Test complete trading cycle"""
        try:
            # Initialize framework
            framework = AIIntegrationFramework()
            await framework.initialize_system()
            
            # Generate signals
            signal = await framework.generate_ai_signal('AAPL')
            
            # Check signal components
            has_ml_predictions = len(signal.ml_predictions) > 0
            has_sentiment = signal.sentiment_signal is not None
            has_combined_signal = signal.combined_action in ['BUY', 'SELL', 'HOLD']
            
            return {
                "full_cycle_completed": True,
                "has_ml_predictions": has_ml_predictions,
                "has_sentiment": has_sentiment,
                "has_combined_signal": has_combined_signal,
                "signal_action": signal.combined_action,
                "signal_confidence": signal.combined_confidence
            }
            
        except Exception as e:
            raise Exception(f"Full trading cycle test failed: {e}")
    
    async def _test_autonomous_system(self) -> Dict[str, Any]:
        """Test autonomous trading system"""
        try:
            framework = AIIntegrationFramework()
            await framework.initialize_system()
            
            # Test system status
            status = framework.get_system_status()
            
            return {
                "autonomous_system_ready": True,
                "system_status": status['system_active'],
                "components_ready": len(status['components'])
            }
            
        except Exception as e:
            raise Exception(f"Autonomous system test failed: {e}")
    
    def _generate_deployment_report(self) -> DeploymentValidationReport:
        """Generate comprehensive deployment report"""
        # Organize tests by category
        tests_by_category = {}
        for category in self.test_categories:
            tests_by_category[category] = [t for t in self.test_results if t.category == category]
        
        # Calculate statistics
        total_tests = len(self.test_results)
        passed_tests = sum(1 for t in self.test_results if t.passed)
        failed_tests = total_tests - passed_tests
        critical_failures = sum(1 for t in self.test_results if not t.passed and t.critical)
        
        # Calculate performance metrics
        performance_metrics = {
            'total_validation_time': time.time() - self.start_time,
            'average_test_time': np.mean([t.duration for t in self.test_results]),
            'success_rate': passed_tests / total_tests if total_tests > 0 else 0,
            'critical_failure_rate': critical_failures / total_tests if total_tests > 0 else 0
        }
        
        # System requirements check
        system_requirements = {
            'python_version': any(t.name == 'python_version' and t.passed for t in self.test_results),
            'package_dependencies': any(t.name == 'package_dependencies' and t.passed for t in self.test_results),
            'system_resources': any(t.name == 'system_resources' and t.passed for t in self.test_results),
            'file_permissions': any(t.name == 'file_permissions' and t.passed for t in self.test_results)
        }
        
        # Security assessment
        security_assessment = {
            'security_framework': any(t.name == 'security_initialization' and t.passed for t in self.test_results),
            'encryption': any(t.name == 'encryption_test' and t.passed for t in self.test_results),
            'authentication': any(t.name == 'authentication_test' and t.passed for t in self.test_results)
        }
        
        # Generate recommendations
        recommendations = self._generate_recommendations()
        
        # Determine overall status
        if critical_failures > 0:
            overall_status = 'FAIL'
            deployment_ready = False
        elif failed_tests > total_tests * 0.2:  # More than 20% failures
            overall_status = 'WARNING'
            deployment_ready = False
        else:
            overall_status = 'PASS'
            deployment_ready = True
        
        return DeploymentValidationReport(
            timestamp=datetime.now(),
            overall_status=overall_status,
            total_tests=total_tests,
            passed_tests=passed_tests,
            failed_tests=failed_tests,
            critical_failures=critical_failures,
            tests_by_category=tests_by_category,
            performance_metrics=performance_metrics,
            system_requirements=system_requirements,
            security_assessment=security_assessment,
            recommendations=recommendations,
            deployment_ready=deployment_ready
        )
    
    def _generate_recommendations(self) -> List[str]:
        """Generate deployment recommendations"""
        recommendations = []
        
        # Check for critical failures
        critical_failures = [t for t in self.test_results if not t.passed and t.critical]
        if critical_failures:
            recommendations.append("‚ùå CRITICAL: Fix critical test failures before deployment")
            for failure in critical_failures:
                recommendations.append(f"   - {failure.name}: {failure.error}")
        
        # Check for performance issues
        slow_tests = [t for t in self.test_results if t.duration > 60]
        if slow_tests:
            recommendations.append("‚ö†Ô∏è  Performance: Some tests are slow, consider optimization")
        
        # Check for missing features
        failed_tests = [t for t in self.test_results if not t.passed and not t.critical]
        if failed_tests:
            recommendations.append("‚ö†Ô∏è  Features: Some non-critical features failed")
        
        # General recommendations
        if not recommendations:
            recommendations.append("‚úÖ System ready for deployment")
        
        recommendations.extend([
            "üîÑ Regular revalidation recommended",
            "üìä Monitor system performance in production",
            "üîí Maintain security best practices",
            "üìà Implement gradual rollout strategy"
        ])
        
        return recommendations
    
    def _create_failed_report(self, error_msg: str) -> DeploymentValidationReport:
        """Create a failed validation report"""
        return DeploymentValidationReport(
            timestamp=datetime.now(),
            overall_status='FAIL',
            total_tests=0,
            passed_tests=0,
            failed_tests=1,
            critical_failures=1,
            tests_by_category={},
            performance_metrics={},
            system_requirements={},
            security_assessment={},
            recommendations=[f"‚ùå Validation failed: {error_msg}"],
            deployment_ready=False
        )
    
    def _save_validation_report(self, report: DeploymentValidationReport):
        """Save validation report to file"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'deployment_validation_{timestamp}.json'
        
        # Convert to JSON-serializable format
        report_dict = {
            'timestamp': report.timestamp.isoformat(),
            'overall_status': report.overall_status,
            'total_tests': report.total_tests,
            'passed_tests': report.passed_tests,
            'failed_tests': report.failed_tests,
            'critical_failures': report.critical_failures,
            'performance_metrics': report.performance_metrics,
            'system_requirements': report.system_requirements,
            'security_assessment': report.security_assessment,
            'recommendations': report.recommendations,
            'deployment_ready': report.deployment_ready,
            'test_results': [
                {
                    'name': t.name,
                    'description': t.description,
                    'category': t.category,
                    'critical': t.critical,
                    'passed': t.passed,
                    'error': t.error,
                    'duration': t.duration,
                    'metadata': t.metadata
                }
                for t in self.test_results
            ]
        }
        
        with open(filename, 'w') as f:
            json.dump(report_dict, f, indent=2)
        
        logger.info(f"üìÑ Validation report saved to {filename}")
    
    def print_validation_report(self, report: DeploymentValidationReport):
        """Print comprehensive validation report"""
        print("\n" + "="*80)
        print("üîç DEPLOYMENT VALIDATION REPORT")
        print("="*80)
        print(f"Generated: {report.timestamp.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"Overall Status: {report.overall_status}")
        print(f"Deployment Ready: {'‚úÖ YES' if report.deployment_ready else '‚ùå NO'}")
        print()
        
        # Test Summary
        print("üìä TEST SUMMARY")
        print("-" * 40)
        print(f"Total Tests: {report.total_tests}")
        print(f"Passed: {report.passed_tests}")
        print(f"Failed: {report.failed_tests}")
        print(f"Critical Failures: {report.critical_failures}")
        print(f"Success Rate: {report.passed_tests/report.total_tests*100:.1f}%")
        print()
        
        # Performance Metrics
        print("‚ö° PERFORMANCE METRICS")
        print("-" * 40)
        for metric, value in report.performance_metrics.items():
            print(f"{metric}: {value:.2f}")
        print()
        
        # System Requirements
        print("üíª SYSTEM REQUIREMENTS")
        print("-" * 40)
        for req, status in report.system_requirements.items():
            status_icon = "‚úÖ" if status else "‚ùå"
            print(f"{status_icon} {req}")
        print()
        
        # Security Assessment
        print("üîí SECURITY ASSESSMENT")
        print("-" * 40)
        for security_item, status in report.security_assessment.items():
            status_icon = "‚úÖ" if status else "‚ùå"
            print(f"{status_icon} {security_item}")
        print()
        
        # Test Results by Category
        print("üî¨ TEST RESULTS BY CATEGORY")
        print("-" * 40)
        for category, tests in report.tests_by_category.items():
            passed = sum(1 for t in tests if t.passed)
            total = len(tests)
            print(f"{category}: {passed}/{total} passed")
            
            # Show failed tests
            failed_tests = [t for t in tests if not t.passed]
            for test in failed_tests:
                criticality = "CRITICAL" if test.critical else "WARNING"
                print(f"   ‚ùå {test.name} ({criticality}): {test.error}")
        print()
        
        # Recommendations
        print("üí° RECOMMENDATIONS")
        print("-" * 40)
        for rec in report.recommendations:
            print(f"{rec}")
        print()
        
        print("="*80)

async def main():
    """Main deployment validation"""
    print("üîç DEPLOYMENT VALIDATION SYSTEM")
    print("=" * 60)
    
    # Initialize validator
    validator = DeploymentValidator()
    
    # Run comprehensive validation
    report = await validator.run_comprehensive_validation()
    
    # Print report
    validator.print_validation_report(report)
    
    # Exit with appropriate code
    if report.deployment_ready:
        print("\nüéâ DEPLOYMENT VALIDATION SUCCESSFUL!")
        print("‚úÖ System is ready for production deployment")
        sys.exit(0)
    else:
        print("\n‚ùå DEPLOYMENT VALIDATION FAILED!")
        print("üîß Fix critical issues before deployment")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main()) 